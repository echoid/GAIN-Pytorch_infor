{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creator: Dhanajit Brahma\n",
    "\n",
    "Adapted from the original implementation in tensorflow from here: https://github.com/jsyoon0823/GAIN\n",
    "\n",
    "Generative Adversarial Imputation Networks (GAIN) Implementation on Letter and Spam Dataset\n",
    "\n",
    "Reference: J. Yoon, J. Jordon, M. van der Schaar, \"GAIN: Missing Data Imputation using Generative Adversarial Nets,\" ICML, 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Packages\n",
    "import torch\n",
    "import numpy as np\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = 'Spam.csv'  # 'Letter.csv' for Letter dataset an 'Spam.csv' for Spam dataset\n",
    "use_gpu = False  # set it to True to use GPU and False to use CPU\n",
    "\n",
    "if use_gpu:\n",
    "    torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_mising(X):\n",
    "    N, D = X.shape\n",
    "    Xnan = X.copy()\n",
    "\n",
    "    mask = np.ones(X.shape)\n",
    "    # ---- MNAR in D/2 dimensions\n",
    "    mean = np.mean(Xnan[:, :int(D / 2)], axis=0)\n",
    "    \n",
    "    ix_larger_than_mean = 1- (Xnan[:, :int(D / 2)] > mean)\n",
    "\n",
    "    mask[:, :int(D / 2)] = ix_larger_than_mean\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000 0.00000000 0.00000000 ... 1.00000000 1.00000000 1.00000000]\n",
      " [0.00000000 0.00000000 0.00000000 ... 1.00000000 1.00000000 1.00000000]\n",
      " [1.00000000 1.00000000 0.00000000 ... 1.00000000 1.00000000 1.00000000]\n",
      " ...\n",
      " [0.00000000 1.00000000 0.00000000 ... 1.00000000 1.00000000 1.00000000]\n",
      " [0.00000000 1.00000000 1.00000000 ... 1.00000000 1.00000000 1.00000000]\n",
      " [1.00000000 1.00000000 0.00000000 ... 1.00000000 1.00000000 1.00000000]]\n"
     ]
    }
   ],
   "source": [
    "#%% System Parameters\n",
    "# 1. Mini batch size\n",
    "mb_size = 32\n",
    "# 2. Missing rate\n",
    "p_miss = 0.5\n",
    "# 3. Hint rate\n",
    "p_hint = 0.9\n",
    "# 4. Loss Hyperparameters\n",
    "alpha = 10\n",
    "# 5. Train Rate\n",
    "train_rate = 0.8\n",
    "\n",
    "#%% Data\n",
    "\n",
    "# Data generation\n",
    "Data = np.loadtxt(dataset_file, delimiter=\",\",skiprows=1)\n",
    "\n",
    "# Parameters\n",
    "No = len(Data)\n",
    "Dim = len(Data[0,:])\n",
    "\n",
    "# Hidden state dimensions\n",
    "H_Dim1 = Dim\n",
    "H_Dim2 = Dim\n",
    "\n",
    "# Normalization (0 to 1)\n",
    "Min_Val = np.zeros(Dim)\n",
    "Max_Val = np.zeros(Dim)\n",
    "\n",
    "for i in range(Dim):\n",
    "    Min_Val[i] = np.min(Data[:,i])\n",
    "    Data[:,i] = Data[:,i] - np.min(Data[:,i])\n",
    "    Max_Val[i] = np.max(Data[:,i])\n",
    "    Data[:,i] = Data[:,i] / (np.max(Data[:,i]) + 1e-6)    \n",
    "\n",
    "#%% Missing introducing\n",
    "\n",
    "Missing = introduce_mising(Data)\n",
    "# p_miss_vec = p_miss * np.ones((Dim,1)) \n",
    "   \n",
    "# Missing = np.zeros((No,Dim))\n",
    "\n",
    "# for i in range(Dim):\n",
    "#     A = np.random.uniform(0., 1., size = [len(Data),])\n",
    "#     B = A > p_miss_vec[i]\n",
    "#     Missing[:,i] = 1.*B\n",
    "\n",
    "    \n",
    "#%% Train Test Division    \n",
    "   \n",
    "idx = np.random.permutation(No)\n",
    "\n",
    "Train_No = int(No * train_rate)\n",
    "Test_No = No - Train_No\n",
    "    \n",
    "# Train / Test Features\n",
    "trainX = Data[idx[:Train_No],:]\n",
    "testX = Data[idx[Train_No:],:]\n",
    "\n",
    "# Train / Test Missing Indicators\n",
    "trainM = Missing[idx[:Train_No],:]\n",
    "testM = Missing[idx[Train_No:],:]\n",
    "\n",
    "#%% Necessary Functions\n",
    "\n",
    "# 1. Xavier Initialization Definition\n",
    "# def xavier_init(size):\n",
    "#     in_dim = size[0]\n",
    "#     xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "#     return tf.random_normal(shape = size, stddev = xavier_stddev)\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / np.sqrt(in_dim / 2.)\n",
    "    return np.random.normal(size = size, scale = xavier_stddev)\n",
    "    \n",
    "# Hint Vector Generation\n",
    "def sample_M(m, n, p):\n",
    "    A = np.random.uniform(0., 1., size = [m, n])\n",
    "    B = A > p\n",
    "    C = 1.*B\n",
    "    return C\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAIN Architecture   \n",
    "GAIN Consists of 3 Components\n",
    "- Generator\n",
    "- Discriminator\n",
    "- Hint Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% 1. Discriminator\n",
    "if use_gpu is True:\n",
    "    D_W1 = torch.tensor(xavier_init([Dim*2, H_Dim1]),requires_grad=True, device=\"cuda\")     # Data + Hint as inputs\n",
    "    D_b1 = torch.tensor(np.zeros(shape = [H_Dim1]),requires_grad=True, device=\"cuda\")\n",
    "\n",
    "    D_W2 = torch.tensor(xavier_init([H_Dim1, H_Dim2]),requires_grad=True, device=\"cuda\")\n",
    "    D_b2 = torch.tensor(np.zeros(shape = [H_Dim2]),requires_grad=True, device=\"cuda\")\n",
    "\n",
    "    D_W3 = torch.tensor(xavier_init([H_Dim2, Dim]),requires_grad=True, device=\"cuda\")\n",
    "    D_b3 = torch.tensor(np.zeros(shape = [Dim]),requires_grad=True, device=\"cuda\")       # Output is multi-variate\n",
    "else:\n",
    "    D_W1 = torch.tensor(xavier_init([Dim*2, H_Dim1]),requires_grad=True)     # Data + Hint as inputs\n",
    "    D_b1 = torch.tensor(np.zeros(shape = [H_Dim1]),requires_grad=True)\n",
    "\n",
    "    D_W2 = torch.tensor(xavier_init([H_Dim1, H_Dim2]),requires_grad=True)\n",
    "    D_b2 = torch.tensor(np.zeros(shape = [H_Dim2]),requires_grad=True)\n",
    "\n",
    "    D_W3 = torch.tensor(xavier_init([H_Dim2, Dim]),requires_grad=True)\n",
    "    D_b3 = torch.tensor(np.zeros(shape = [Dim]),requires_grad=True)       # Output is multi-variate\n",
    "\n",
    "theta_D = [D_W1, D_W2, D_W3, D_b1, D_b2, D_b3]\n",
    "\n",
    "#%% 2. Generator\n",
    "if use_gpu is True:\n",
    "    G_W1 = torch.tensor(xavier_init([Dim*2, H_Dim1]),requires_grad=True, device=\"cuda\")     # Data + Mask as inputs (Random Noises are in Missing Components)\n",
    "    G_b1 = torch.tensor(np.zeros(shape = [H_Dim1]),requires_grad=True, device=\"cuda\")\n",
    "\n",
    "    G_W2 = torch.tensor(xavier_init([H_Dim1, H_Dim2]),requires_grad=True, device=\"cuda\")\n",
    "    G_b2 = torch.tensor(np.zeros(shape = [H_Dim2]),requires_grad=True, device=\"cuda\")\n",
    "\n",
    "    G_W3 = torch.tensor(xavier_init([H_Dim2, Dim]),requires_grad=True, device=\"cuda\")\n",
    "    G_b3 = torch.tensor(np.zeros(shape = [Dim]),requires_grad=True, device=\"cuda\")\n",
    "else:\n",
    "    G_W1 = torch.tensor(xavier_init([Dim*2, H_Dim1]),requires_grad=True)     # Data + Mask as inputs (Random Noises are in Missing Components)\n",
    "    G_b1 = torch.tensor(np.zeros(shape = [H_Dim1]),requires_grad=True)\n",
    "\n",
    "    G_W2 = torch.tensor(xavier_init([H_Dim1, H_Dim2]),requires_grad=True)\n",
    "    G_b2 = torch.tensor(np.zeros(shape = [H_Dim2]),requires_grad=True)\n",
    "\n",
    "    G_W3 = torch.tensor(xavier_init([H_Dim2, Dim]),requires_grad=True)\n",
    "    G_b3 = torch.tensor(np.zeros(shape = [Dim]),requires_grad=True)\n",
    "\n",
    "theta_G = [G_W1, G_W2, G_W3, G_b1, G_b2, G_b3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAIN Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% 1. Generator\n",
    "def generator(new_x,m):\n",
    "    inputs = torch.cat(dim = 1, tensors = [new_x,m])  # Mask + Data Concatenate\n",
    "    G_h1 = F.relu(torch.matmul(inputs, G_W1) + G_b1)\n",
    "    G_h2 = F.relu(torch.matmul(G_h1, G_W2) + G_b2)   \n",
    "    G_prob = torch.sigmoid(torch.matmul(G_h2, G_W3) + G_b3) # [0,1] normalized Output\n",
    "    \n",
    "    return G_prob\n",
    "\n",
    "#%% 2. Discriminator\n",
    "def discriminator(new_x, h):\n",
    "    inputs = torch.cat(dim = 1, tensors = [new_x,h])  # Hint + Data Concatenate\n",
    "    D_h1 = F.relu(torch.matmul(inputs, D_W1) + D_b1)  \n",
    "    D_h2 = F.relu(torch.matmul(D_h1, D_W2) + D_b2)\n",
    "    D_logit = torch.matmul(D_h2, D_W3) + D_b3\n",
    "    D_prob = torch.sigmoid(D_logit)  # [0,1] Probability Output\n",
    "    \n",
    "    return D_prob\n",
    "\n",
    "#%% 3. Other functions\n",
    "# Random sample generator for Z\n",
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(0., 0.01, size = [m, n])        \n",
    "\n",
    "# Mini-batch generation\n",
    "def sample_idx(m, n):\n",
    "    A = np.random.permutation(m)\n",
    "    idx = A[:n]\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_columnwise_kl(real_data, fake_data):\n",
    "    kl_divergences = []\n",
    "    \n",
    "    for col_idx in range(real_data.shape[1]):  # Iterate over columns\n",
    "        real_column = real_data[:, col_idx]\n",
    "        fake_column = fake_data[:, col_idx]\n",
    "        \n",
    "        # Calculate mean and covariance for each distribution\n",
    "        mean_real = torch.mean(real_column)\n",
    "        cov_real = torch.diag(torch.var(real_column, unbiased=False))\n",
    "        \n",
    "        mean_fake = torch.mean(fake_column)\n",
    "        cov_fake = torch.diag(torch.var(fake_column, unbiased=False))\n",
    "        \n",
    "        # Calculate KL divergence using the formula\n",
    "        kl_div = 0.5 * (torch.trace(torch.inverse(cov_fake) @ cov_real) +\n",
    "                        (mean_fake - mean_real).T @ torch.inverse(cov_fake) @ (mean_fake - mean_real) -\n",
    "                        real_data.shape[1] + torch.logdet(cov_fake) - torch.logdet(cov_real))\n",
    "        \n",
    "        kl_divergences.append(kl_div.item())\n",
    "    \n",
    "    return kl_divergences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2732, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "input = F.log_softmax(torch.randn(3, 5, requires_grad=True), dim=1)\n",
    "target = F.softmax(torch.rand(3, 5), dim=1)\n",
    "kl_loss = F.kl_div(input, target,reduction=\"batchmean\")\n",
    "print(kl_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1071,  0.5157,  1.3424,  0.8551, -1.1064],\n",
       "        [-0.2424, -1.9204,  0.6511,  0.6910,  1.8144],\n",
       "        [ 0.5413, -0.8244, -3.0493,  0.3709, -1.5368]], requires_grad=True)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3, 5, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.randn(3, 5, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.kl_div(torch.randn(3, 5, requires_grad=True), torch.randn(3, 5, requires_grad=True),reduction=\"batchmean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAIN Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(M, New_X, H):\n",
    "    # Generator\n",
    "    G_sample = generator(New_X,M)\n",
    "    # Combine with original data\n",
    "    Hat_New_X = New_X * M + G_sample * (1-M)\n",
    "\n",
    "    # Discriminator\n",
    "    D_prob = discriminator(Hat_New_X, H)\n",
    "\n",
    "    #%% Loss\n",
    "    D_loss = -torch.mean(M * torch.log(D_prob + 1e-8) + (1-M) * torch.log(1. - D_prob + 1e-8))\n",
    "    return D_loss\n",
    "\n",
    "def generator_loss(X, M, New_X, H):\n",
    "    #%% Structure\n",
    "    # Generator\n",
    "    kl_loss = torch.mean(torch.tensor([0,0,0]).float())\n",
    "    G_sample = generator(New_X,M)\n",
    "\n",
    "    # Combine with original data\n",
    "    Hat_New_X = New_X * M + G_sample * (1-M)\n",
    "\n",
    "    # Discriminator\n",
    "    D_prob = discriminator(Hat_New_X, H)\n",
    "\n",
    "    #%% Loss\n",
    "    G_loss1 = -torch.mean((1-M) * torch.log(D_prob + 1e-8))\n",
    "    MSE_train_loss = torch.mean((M * New_X - M * G_sample)**2) / torch.mean(M)\n",
    "\n",
    "    #kl_loss = F.kl_div(torch.log(X), torch.log(G_sample))\n",
    "\n",
    "    print(kl_loss,G_loss1,MSE_train_loss)\n",
    "    print(G_loss1,MSE_train_loss)\n",
    "\n",
    "    G_loss = G_loss1 + alpha * MSE_train_loss + 0.01 * kl_loss\n",
    "\n",
    "    #%% MSE Performance metric\n",
    "    MSE_test_loss = torch.mean(((1-M) * X - (1-M)*G_sample)**2) / torch.mean(1-M)\n",
    "    return G_loss, MSE_train_loss, MSE_test_loss,kl_loss\n",
    "    \n",
    "def test_loss(X, M, New_X):\n",
    "    #%% Structure\n",
    "    # Generator\n",
    "    G_sample = generator(New_X,M)\n",
    "\n",
    "    #%% MSE Performance metric\n",
    "    MSE_test_loss = torch.mean(((1-M) * X - (1-M)*G_sample)**2) / torch.mean(1-M)\n",
    "    return MSE_test_loss, G_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_D = torch.optim.Adam(params=theta_D)\n",
    "optimizer_G = torch.optim.Adam(params=theta_G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e8cb14a6ea4c43a5dac9edbdbe2b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.) tensor(0.0491, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.2368, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0491, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.2368, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Iter: 0\tTrain_loss: 0.4866\tTest_loss: 0.3873\n",
      "KL_loss: 0.0\n",
      "tensor(0.) tensor(0.0719, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.2227, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0719, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.2227, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0526, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.2115, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0526, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.2115, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0560, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.2011, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0560, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.2011, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0511, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.1902, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0511, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.1902, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0528, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.1814, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0528, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.1814, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0645, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.1684, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0645, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.1684, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0618, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.1593, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0618, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.1593, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0632, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.1500, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0632, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.1500, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0529, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.1405, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0529, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.1405, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0455, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.1296, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0455, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.1296, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Iter: 10\tTrain_loss: 0.36\tTest_loss: 0.2605\n",
      "KL_loss: 0.0\n",
      "tensor(0.) tensor(0.0546, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.1209, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0546, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.1209, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0482, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.1124, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0482, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.1124, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0718, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.1056, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0718, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.1056, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0358, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0944, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0358, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0944, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0567, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0882, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0567, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0882, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0492, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0808, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0492, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0808, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0490, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0736, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0490, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0736, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0620, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0672, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0620, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0672, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0580, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0631, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0580, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0631, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0569, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0559, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0569, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0559, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Iter: 20\tTrain_loss: 0.2363\tTest_loss: 0.1848\n",
      "KL_loss: 0.0\n",
      "tensor(0.) tensor(0.0421, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0485, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0421, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0485, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0469, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0432, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0469, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0432, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0444, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0376, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0444, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0376, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0535, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0352, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0535, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0352, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0426, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0306, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0426, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0306, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0418, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0255, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0418, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0255, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0347, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0223, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0347, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0223, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0373, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0190, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0373, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0190, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0383, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0169, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0383, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0169, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0298, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0147, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0298, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0147, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Iter: 30\tTrain_loss: 0.1214\tTest_loss: 0.1446\n",
      "KL_loss: 0.0\n",
      "tensor(0.) tensor(0.0326, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0121, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0326, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0121, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0340, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0103, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0340, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0103, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0295, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0086, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0295, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0086, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0348, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0086, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0348, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0086, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0253, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0066, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0253, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0066, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0246, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0052, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0246, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0052, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0288, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0049, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0288, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0049, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0189, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0039, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0189, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0039, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0225, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0040, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0225, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0040, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0269, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0038, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0269, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0038, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Iter: 40\tTrain_loss: 0.06127\tTest_loss: 0.1199\n",
      "KL_loss: 0.0\n",
      "tensor(0.) tensor(0.0237, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0029, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0237, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0029, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0227, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0023, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0227, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0023, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0220, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0023, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0220, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0023, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0200, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0026, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0200, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0026, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0225, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0039, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0225, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0039, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0267, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0047, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0267, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0047, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0272, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0015, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0272, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0015, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0261, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0013, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0261, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0013, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0184, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0033, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0184, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0033, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0268, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0008, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0268, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0008, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Iter: 50\tTrain_loss: 0.02857\tTest_loss: 0.166\n",
      "KL_loss: 0.0\n",
      "tensor(0.) tensor(0.0184, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0008, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0184, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0008, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0203, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0016, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0203, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0016, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0242, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0012, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0242, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0012, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0203, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0009, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0203, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0009, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0278, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0006, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0278, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0006, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0221, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0005, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0221, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0005, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0205, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0015, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0205, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0015, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0238, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0012, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0238, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0012, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0243, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0010, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0243, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0010, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0233, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0014, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0233, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0014, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Iter: 60\tTrain_loss: 0.03722\tTest_loss: 0.1902\n",
      "KL_loss: 0.0\n",
      "tensor(0.) tensor(0.0274, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0011, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0274, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0011, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0271, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0007, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0271, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0007, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0279, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0012, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0279, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0012, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0270, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0007, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0270, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0007, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0275, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0015, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0275, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0015, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0253, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0012, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0253, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0012, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0256, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0009, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0256, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0009, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0346, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0011, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0346, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0011, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0321, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0010, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0321, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0010, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0306, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0061, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0306, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0061, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Iter: 70\tTrain_loss: 0.07822\tTest_loss: 0.1862\n",
      "KL_loss: 0.0\n",
      "tensor(0.) tensor(0.0245, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0008, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0245, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0008, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0241, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0010, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0241, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0010, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0323, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0010, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0323, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0010, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0308, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0008, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0308, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0008, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0331, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0009, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0331, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0009, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0216, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0020, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0216, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0020, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0213, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0005, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0213, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0005, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0331, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0009, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0331, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0009, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0216, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0012, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0216, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0012, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0282, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0017, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0282, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0017, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Iter: 80\tTrain_loss: 0.04152\tTest_loss: 0.156\n",
      "KL_loss: 0.0\n",
      "tensor(0.) tensor(0.0326, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0014, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0326, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0014, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0252, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0013, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0252, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0013, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0266, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0011, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0266, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0011, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0201, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0007, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0201, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0007, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0207, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0005, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0207, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0005, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0338, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0032, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0338, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0032, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0326, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0028, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0326, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0028, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0247, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0005, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0247, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0005, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0335, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0004, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0335, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0004, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0369, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0011, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0369, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0011, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Iter: 90\tTrain_loss: 0.03245\tTest_loss: 0.1677\n",
      "KL_loss: 0.0\n",
      "tensor(0.) tensor(0.0310, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0016, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0310, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0016, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0363, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0018, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0363, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0018, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0360, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0015, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0360, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0015, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0388, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0028, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0388, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0028, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0357, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0014, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0357, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0014, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0400, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0005, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0400, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0005, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0397, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0010, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0397, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0010, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0241, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0014, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0241, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0014, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.) tensor(0.0386, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0007, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "tensor(0.0386, dtype=torch.float64, grad_fn=<NegBackward0>) tensor(0.0007, dtype=torch.float64, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#%% Start Iterations\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_kl = []\n",
    "iterations = []\n",
    "\n",
    "for it in tqdm(range(100)):    \n",
    "    \n",
    "    #%% Inputs\n",
    "    mb_idx = sample_idx(Train_No, mb_size)\n",
    "    X_mb = trainX[mb_idx,:]  \n",
    "    \n",
    "    Z_mb = sample_Z(mb_size, Dim) \n",
    "    M_mb = trainM[mb_idx,:]  \n",
    "    H_mb1 = sample_M(mb_size, Dim, 1-p_hint)\n",
    "    H_mb = M_mb * H_mb1\n",
    "    \n",
    "    New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb  # Missing Data Introduce\n",
    "    \n",
    "    if use_gpu is True:\n",
    "        X_mb = torch.tensor(X_mb, device=\"cuda\")\n",
    "        M_mb = torch.tensor(M_mb, device=\"cuda\")\n",
    "        H_mb = torch.tensor(H_mb, device=\"cuda\")\n",
    "        New_X_mb = torch.tensor(New_X_mb, device=\"cuda\")\n",
    "    else:\n",
    "        X_mb = torch.tensor(X_mb)\n",
    "        M_mb = torch.tensor(M_mb)\n",
    "        H_mb = torch.tensor(H_mb)\n",
    "        New_X_mb = torch.tensor(New_X_mb)\n",
    "    \n",
    "    optimizer_D.zero_grad()\n",
    "    D_loss_curr = discriminator_loss(M=M_mb, New_X=New_X_mb, H=H_mb)\n",
    "    D_loss_curr.backward()\n",
    "    optimizer_D.step()\n",
    "    \n",
    "    optimizer_G.zero_grad()\n",
    "    G_loss_curr, MSE_train_loss_curr, MSE_test_loss_curr, kl_loss = generator_loss(X=X_mb, M=M_mb, New_X=New_X_mb, H=H_mb)\n",
    "    G_loss_curr.backward()\n",
    "    optimizer_G.step()    \n",
    "        \n",
    "    #%% Intermediate Losses\n",
    "    if it % 10 == 0:\n",
    "        print('Iter: {}'.format(it),end='\\t')\n",
    "        print('Train_loss: {:.4}'.format(np.sqrt(MSE_train_loss_curr.item())),end='\\t')\n",
    "        print('Test_loss: {:.4}'.format(np.sqrt(MSE_test_loss_curr.item())))\n",
    "        print('KL_loss: {:.4}'.format(np.sqrt(kl_loss.item())))\n",
    "\n",
    "\n",
    "\n",
    "            # Append the current iteration and losses to the lists\n",
    "        iterations.append(it)\n",
    "        train_losses.append(np.sqrt(MSE_train_loss_curr.item()))\n",
    "        test_losses.append(np.sqrt(MSE_test_loss_curr.item()))\n",
    "        train_kl.append(np.sqrt(kl_loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a figure and a set of subplots\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot train losses\n",
    "ax.plot(iterations, train_losses, label='Train Loss', color='blue')\n",
    "\n",
    "# Plot test losses\n",
    "ax.plot(iterations, test_losses, label='Test Loss', color='red')\n",
    "\n",
    "# Plot test losses\n",
    "ax.plot(iterations, train_kl, label='Train KL', color='m')\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Intermediate Train and Test Losses')\n",
    "\n",
    "# Add a legend\n",
    "ax.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test RMSE: 0.1697991279695795\n"
     ]
    }
   ],
   "source": [
    "Z_mb = sample_Z(Test_No, Dim) \n",
    "M_mb = testM\n",
    "X_mb = testX\n",
    "        \n",
    "New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb  # Missing Data Introduce\n",
    "\n",
    "if use_gpu is True:\n",
    "    X_mb = torch.tensor(X_mb, device='cuda')\n",
    "    M_mb = torch.tensor(M_mb, device='cuda')\n",
    "    New_X_mb = torch.tensor(New_X_mb, device='cuda')\n",
    "else:\n",
    "    X_mb = torch.tensor(X_mb)\n",
    "    M_mb = torch.tensor(M_mb)\n",
    "    New_X_mb = torch.tensor(New_X_mb)\n",
    "    \n",
    "MSE_final, Sample = test_loss(X=X_mb, M=M_mb, New_X=New_X_mb)\n",
    "        \n",
    "print('Final Test RMSE: ' + str(np.sqrt(MSE_final.item())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data = M_mb * X_mb + (1-M_mb) * Sample\n",
    "print(\"Imputed test data:\")\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.8f}\".format(x)})\n",
    "\n",
    "if use_gpu is True:\n",
    "    print(imputed_data.cpu().detach().numpy())\n",
    "else:\n",
    "    print(imputed_data.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
